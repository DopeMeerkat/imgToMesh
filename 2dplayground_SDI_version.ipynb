{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate 2D Images using SDI\n",
    "\n",
    "This notebook demonstrates how to generate 2D images using SDI with 2 different approaches.\n",
    "\n",
    "1. **Cache $\\kappa_y^t$**\n",
    "\n",
    "    Unlike 3D case, in 2D we can keep the history of the previously predicted noise and use it as $\\kappa_y^t$ from eq.10 in the paper.\n",
    "    This is the optimal solution, as unlike DDIM inversion it gives the exact solution of eq.8.\n",
    "    \n",
    "    *Use this method if the image does NOT get updated in-between the SDI iterations.*\n",
    "\n",
    "2. **DDIM inversion**\n",
    "\n",
    "    If the image gets updated in-between the SDI iterations, $\\kappa_y^t$ from the previous method becomes invalid.\n",
    "    In this case, we can use DDIM inversion to estimate $\\kappa_y^t$ from the previous image and the current image.\n",
    "\n",
    "    *Use this method if the image gets updated or you need smaller LRs.*\n",
    "\n",
    "At first, lets set everything up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = \"/mnt/arteml/data/HugginFacesCache\"\n",
    "\n",
    "# NOTE: It is important to explicitly define a GPU. \n",
    "# There is a bug in certain versions of the transformers library that can mess up the CLIP embeddings if the GPU is not explicitly specified.\n",
    "os.environ['CUDA_VISIBLE_DEVICE'] = \"7\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import threestudio\n",
    "import gc\n",
    "import time\n",
    "import io\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, IntSlider, Output\n",
    "from IPython.display import display, clear_output\n",
    "from PIL import Image\n",
    "\n",
    "def get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps, num_cycles: float = 0.5):\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress)))\n",
    "\n",
    "    return LambdaLR(optimizer, lr_lambda, -1)\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    \n",
    "def figure2image(fig):\n",
    "    buf = io.BytesIO()\n",
    "    fig.savefig(buf)\n",
    "    buf.seek(0)\n",
    "    img = Image.open(buf)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"bagel filled with cream cheese and lox\"\n",
    "n_iters = 1000\n",
    "\n",
    "config = {\n",
    "    'max_iters': n_iters,\n",
    "    'seed': 0,\n",
    "    'scheduler': None,\n",
    "    'mode': 'latent',\n",
    "    'prompt_processor_type': 'stable-diffusion-prompt-processor',\n",
    "    'prompt_processor': {\n",
    "        'pretrained_model_name_or_path': \"stabilityai/stable-diffusion-2-1-base\",\n",
    "        'prompt': prompt,\n",
    "        'use_perp_neg': False,\n",
    "    },\n",
    "    'guidance_type': 'stable-diffusion-sdi-guidance',\n",
    "    'guidance': {\n",
    "        'pretrained_model_name_or_path': \"stabilityai/stable-diffusion-2-1-base\",\n",
    "        'guidance_scale': 7.5,\n",
    "        'weighting_strategy': \"sds\",\n",
    "        'min_step_percent': 0.02,\n",
    "        'max_step_percent': 0.98,\n",
    "        'trainer_max_steps': n_iters,\n",
    "        'grad_clip': None,\n",
    "        # SDI parameters\n",
    "        'enable_sdi': True,\n",
    "        'inversion_guidance_scale': -7.5,\n",
    "        'inversion_n_steps': 10,\n",
    "        'inversion_eta': 0.3,\n",
    "        't_anneal': True\n",
    "    },\n",
    "    'latent': {\n",
    "        'width': 64,\n",
    "        'height': 64,\n",
    "    }\n",
    "}\n",
    "\n",
    "batch = {\n",
    "    'elevation': torch.Tensor([0]),\n",
    "    'azimuth': torch.Tensor([0]),\n",
    "    'camera_distances': torch.Tensor([1]),\n",
    "}\n",
    "\n",
    "seed_everything(config['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just need to rerun the cell when you change guidance or prompt_processor\n",
    "guidance = None\n",
    "prompt_processor = None\n",
    "gc.collect()\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "guidance = threestudio.find(config['guidance_type'])(config['guidance'])\n",
    "prompt_processor = threestudio.find(config['prompt_processor_type'])(config['prompt_processor'])\n",
    "# prompt_processor.configure_text_encoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. **Cache $\\kappa_y^t$**\n",
    "\n",
    "Unlike 3D case, in 2D we can keep the history of the previously predicted noise and use it as $\\kappa_y^t$ from eq.10 in the paper.\n",
    "This is the optimal solution, as unlike DDIM inversion it gives the exact solution of eq.8.\n",
    "Note, that since we are still doing optimization in between the update steps, the noise becomes a little bit imperfect.\n",
    "\n",
    "If you want to achieve the exact match to DDIM, you can assigne latent to the predicted target directly instead of running optimization: \n",
    "```python\n",
    "latent = quiadance_output[\"target_latent\"]\n",
    "```\n",
    "\n",
    "*This method is faster and more precise then DDIM inversion, but would not work if the images are being updated in between the iterations.*\n",
    "\n",
    "**Use this method if the image does NOT get updated in-between the SDI iterations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 1\n",
    "lr = 1e-1\n",
    "t_n_steps = 40\n",
    "\n",
    "gc.collect()\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "w, h = config['latent']['width'], config['latent']['height']\n",
    "\n",
    "\n",
    "# Initialization\n",
    "time = torch.tensor(B * [999], device=guidance.device)\n",
    "init_noise = torch.randn(B, 4, h, w, device=guidance.device) * guidance.scheduler.init_noise_sigma\n",
    "noise_pred, _, _ = guidance.predict_noise(\n",
    "    init_noise,\n",
    "    time,\n",
    "    prompt_processor(),\n",
    "    **batch,\n",
    "    guidance_scale=config['guidance']['guidance_scale']\n",
    ")\n",
    "latents_denoised = guidance.get_x0(init_noise, noise_pred, time).detach()\n",
    "latent = nn.Parameter(latents_denoised)\n",
    "\n",
    "optimizer = torch.optim.Adam([latent], lr=lr, weight_decay=0)\n",
    "num_steps = config['max_iters']\n",
    "t_interval = n_iters // t_n_steps\n",
    "img_array = []\n",
    "\n",
    "try:\n",
    "    for step in tqdm(range(num_steps + 1)):\n",
    "        guidance.update_step(epoch=0, global_step=step)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if step % t_interval == 0:\n",
    "            guidance_output = guidance(\n",
    "                latent,\n",
    "                prompt_processor(), **batch, test_info=True, rgb_as_latents=True,\n",
    "                call_with_defined_noise=noise_pred\n",
    "            )\n",
    "        \n",
    "        loss = 0.5 * F.mse_loss(latent, guidance_output['target_latent'].detach(), reduction=\"mean\")\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        noise_pred = guidance_output[\"noise_pred\"]\n",
    "        \n",
    "        if step % 5 == 0:\n",
    "            rgb = guidance.decode_latents(latent).permute(0, 2, 3, 1)\n",
    "            img_rgb = rgb.clamp(0, 1).detach().squeeze(0).cpu().numpy()\n",
    "\n",
    "            fig, ax = plt.subplots(1, 4, figsize=(20, 5))\n",
    "            ax[0].imshow(img_rgb)\n",
    "            ax[1].imshow(guidance_output[\"target\"].cpu().numpy())\n",
    "            ax[2].imshow(guidance_output[\"noisy_img\"].cpu().numpy())\n",
    "            ax[3].imshow(guidance_output[\"noise_img\"].cpu().numpy())\n",
    "            ax[0].set_title('Current Image')\n",
    "            ax[1].set_title('Target Image')\n",
    "            ax[2].set_title('Noisy Image')\n",
    "            ax[3].set_title('Noise')\n",
    "            ax[0].axis('off')\n",
    "            ax[1].axis('off')\n",
    "            ax[2].axis('off')\n",
    "            ax[3].axis('off')\n",
    "            clear_output(wait=True)\n",
    "            plt.show()\n",
    "            img_array.append(figure2image(fig))\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    # browse the result\n",
    "    print(\"Optimizing process:\")\n",
    "    images = img_array\n",
    "    \n",
    "    if len(images) > 0:\n",
    "        # Set up the widgets\n",
    "        slider = IntSlider(min=0, max=len(images)-1, step=1, value=1)\n",
    "        output = Output()\n",
    "\n",
    "        def display_image(index):\n",
    "            with output:\n",
    "                output.clear_output(wait=True)\n",
    "                display(images[index])\n",
    "        \n",
    "        # Link the slider to the display function\n",
    "        interact(display_image, index=slider)\n",
    "\n",
    "        # Display the widgets\n",
    "        # display(slider)\n",
    "        display(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. **DDIM inversion**\n",
    "This is the same approach we use in 3D.\n",
    "\n",
    "If the image gets updated in-between the SDI iterations, $\\kappa_y^t$ from the previous method becomes invalid.\n",
    "In this case, we can use DDIM inversion to estimate $\\kappa_y^t$ from the previous image and the current image. This is the same approach we use in 3D.\n",
    "\n",
    "*Use this method if the image gets updated or you need smaller LRs.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 1\n",
    "lr = 1e-1\n",
    "t_n_steps = 40\n",
    "\n",
    "gc.collect()\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "w, h = config['latent']['width'], config['latent']['height']\n",
    "\n",
    "# Initialization\n",
    "time = torch.tensor(B * [999], device=guidance.device)\n",
    "init_noise = torch.randn(B, 4, h, w, device=guidance.device) * guidance.scheduler.init_noise_sigma\n",
    "noise_pred, _, _ = guidance.predict_noise(\n",
    "    init_noise,\n",
    "    time,\n",
    "    prompt_processor(),\n",
    "    **batch,\n",
    "    guidance_scale=config['guidance']['guidance_scale']\n",
    ")\n",
    "latents_denoised = guidance.get_x0(init_noise, noise_pred, time).detach()\n",
    "latent = nn.Parameter(latents_denoised)\n",
    "\n",
    "optimizer = torch.optim.Adam([latent], lr=lr, weight_decay=0)\n",
    "num_steps = config['max_iters']\n",
    "t_interval = n_iters // t_n_steps\n",
    "img_array = []\n",
    "\n",
    "try:\n",
    "    for step in tqdm(range(num_steps + 1)):\n",
    "        guidance.update_step(epoch=0, global_step=step)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if step % t_interval == 0:\n",
    "            guidance_output = guidance(\n",
    "                guidance.decode_latents(latent).permute(0, 2, 3, 1), # project through the decoder to regulirize the latents\n",
    "                prompt_processor(), **batch, test_info=True # rgb_as_latents=True, \n",
    "            )\n",
    "        \n",
    "        loss = 0.5 * F.mse_loss(latent, guidance_output['target_latent'].detach(), reduction=\"mean\")\n",
    "        loss.backward()\n",
    "        # guidance_output[\"loss_sdi\"].backward()\n",
    "        optimizer.step()\n",
    "        noise_pred = guidance_output[\"noise_pred\"]\n",
    "        \n",
    "        if step % 5 == 0:\n",
    "            rgb = guidance.decode_latents(latent).permute(0, 2, 3, 1)\n",
    "            img_rgb = rgb.clamp(0, 1).detach().squeeze(0).cpu().numpy()\n",
    "\n",
    "            fig, ax = plt.subplots(1, 4, figsize=(20, 5))\n",
    "            ax[0].imshow(img_rgb)\n",
    "            ax[1].imshow(guidance_output[\"target\"].cpu().numpy())\n",
    "            ax[2].imshow(guidance_output[\"noisy_img\"].cpu().numpy())\n",
    "            ax[3].imshow(guidance_output[\"noise_img\"].cpu().numpy())\n",
    "            ax[0].set_title('Current Image')\n",
    "            ax[1].set_title('Target Image')\n",
    "            ax[2].set_title('Noisy Image')\n",
    "            ax[3].set_title('Noise')\n",
    "            ax[0].axis('off')\n",
    "            ax[1].axis('off')\n",
    "            ax[2].axis('off')\n",
    "            ax[3].axis('off')\n",
    "            clear_output(wait=True)\n",
    "            plt.show()\n",
    "            img_array.append(figure2image(fig))\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    # browse the result\n",
    "    print(\"Optimizing process:\")\n",
    "    images = img_array\n",
    "    \n",
    "    if len(images) > 0:\n",
    "        # Set up the widgets\n",
    "        slider = IntSlider(min=0, max=len(images)-1, step=1, value=1)\n",
    "        output = Output()\n",
    "\n",
    "        def display_image(index):\n",
    "            with output:\n",
    "                output.clear_output(wait=True)\n",
    "                display(images[index])\n",
    "        \n",
    "        # Link the slider to the display function\n",
    "        interact(display_image, index=slider)\n",
    "\n",
    "        # Display the widgets\n",
    "        # display(slider)\n",
    "        display(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
